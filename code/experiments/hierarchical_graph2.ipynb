{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","mount_file_id":"1Wagrqpm82jkR23k6cPC0jI-ib12jLvPF","authorship_tag":"ABX9TyP1QwWvjka7f1iY0tG/je7w"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"TPU"},"cells":[{"cell_type":"markdown","source":["paragraph level node → select n important paragraphs → determine weight of each paragraph(1 < weight < 5)"],"metadata":{"id":"KPbdqi99pDPK"}},{"cell_type":"code","source":["!pip install cdlib stanza rouge zss"],"metadata":{"id":"qVJk8jfgpaMt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670299795516,"user_tz":-540,"elapsed":18157,"user":{"displayName":"박준영","userId":"16803711589753004830"}},"outputId":"ec1bad7a-7ff3-4444-ccef-b35405352078"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting cdlib\n","  Downloading cdlib-0.2.6-py3-none-any.whl (228 kB)\n","\u001b[K     |████████████████████████████████| 228 kB 4.9 MB/s \n","\u001b[?25hCollecting stanza\n","  Downloading stanza-1.4.2-py3-none-any.whl (691 kB)\n","\u001b[K     |████████████████████████████████| 691 kB 73.8 MB/s \n","\u001b[?25hCollecting rouge\n","  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n","Collecting zss\n","  Downloading zss-1.2.0.tar.gz (9.8 kB)\n","Requirement already satisfied: pooch in /usr/local/lib/python3.8/dist-packages (from cdlib) (1.6.0)\n","Collecting dynetx\n","  Downloading dynetx-0.3.1-py3-none-any.whl (39 kB)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from cdlib) (4.64.1)\n","Requirement already satisfied: cython in /usr/local/lib/python3.8/dist-packages (from cdlib) (0.29.32)\n","Collecting eva-lcd\n","  Downloading eva_lcd-0.1.1-py3-none-any.whl (9.2 kB)\n","Collecting demon\n","  Downloading demon-2.0.6-py3-none-any.whl (7.3 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from cdlib) (1.21.6)\n","Collecting python-Levenshtein\n","  Downloading python_Levenshtein-0.20.8-py3-none-any.whl (9.4 kB)\n","Collecting chinese-whispers\n","  Downloading chinese_whispers-0.8.0-py3-none-any.whl (7.7 kB)\n","Requirement already satisfied: python-louvain>=0.16 in /usr/local/lib/python3.8/dist-packages (from cdlib) (0.16)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from cdlib) (1.3.5)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from cdlib) (3.2.2)\n","Collecting pyclustering\n","  Downloading pyclustering-0.10.1.2.tar.gz (2.6 MB)\n","\u001b[K     |████████████████████████████████| 2.6 MB 60.9 MB/s \n","\u001b[?25hCollecting pulp\n","  Downloading PuLP-2.7.0-py3-none-any.whl (14.3 MB)\n","\u001b[K     |████████████████████████████████| 14.3 MB 48.4 MB/s \n","\u001b[?25hRequirement already satisfied: seaborn in /usr/local/lib/python3.8/dist-packages (from cdlib) (0.11.2)\n","Requirement already satisfied: future in /usr/local/lib/python3.8/dist-packages (from cdlib) (0.16.0)\n","Collecting thresholdclustering\n","  Downloading thresholdclustering-1.1-py3-none-any.whl (5.3 kB)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from cdlib) (1.7.3)\n","Collecting bimlpa\n","  Downloading bimlpa-0.1.2-py3-none-any.whl (7.0 kB)\n","Collecting angel-cd\n","  Downloading angel_cd-1.0.3-py3-none-any.whl (10 kB)\n","Collecting python-igraph\n","  Downloading python-igraph-0.10.2.tar.gz (9.5 kB)\n","Requirement already satisfied: networkx>=2.4 in /usr/local/lib/python3.8/dist-packages (from cdlib) (2.6.3)\n","Collecting nf1\n","  Downloading nf1-0.0.4-py3-none-any.whl (18 kB)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from cdlib) (1.0.2)\n","Collecting markov-clustering\n","  Downloading markov_clustering-0.0.6.dev0-py3-none-any.whl (6.3 kB)\n","Collecting emoji\n","  Downloading emoji-2.2.0.tar.gz (240 kB)\n","\u001b[K     |████████████████████████████████| 240 kB 77.6 MB/s \n","\u001b[?25hRequirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.8/dist-packages (from stanza) (1.12.1+cu113)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.8/dist-packages (from stanza) (3.19.6)\n","Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from stanza) (1.15.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from stanza) (2.23.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.3.0->stanza) (4.1.1)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from dynetx->cdlib) (4.4.2)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->cdlib) (2.8.2)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->cdlib) (1.4.4)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->cdlib) (0.11.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->cdlib) (3.0.9)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->cdlib) (2022.6)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from pooch->cdlib) (21.3)\n","Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.8/dist-packages (from pooch->cdlib) (1.4.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->stanza) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->stanza) (2022.9.24)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->stanza) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->stanza) (2.10)\n","Requirement already satisfied: Pillow>=5.2.0 in /usr/local/lib/python3.8/dist-packages (from pyclustering->cdlib) (7.1.2)\n","Collecting igraph==0.10.2\n","  Downloading igraph-0.10.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 68.6 MB/s \n","\u001b[?25hCollecting texttable>=1.6.2\n","  Downloading texttable-1.6.7-py2.py3-none-any.whl (10 kB)\n","Collecting Levenshtein==0.20.8\n","  Downloading Levenshtein-0.20.8-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (174 kB)\n","\u001b[K     |████████████████████████████████| 174 kB 87.7 MB/s \n","\u001b[?25hCollecting rapidfuzz<3.0.0,>=2.3.0\n","  Downloading rapidfuzz-2.13.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n","\u001b[K     |████████████████████████████████| 2.2 MB 71.1 MB/s \n","\u001b[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->cdlib) (3.1.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->cdlib) (1.2.0)\n","Building wheels for collected packages: zss, emoji, pyclustering, python-igraph\n","  Building wheel for zss (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for zss: filename=zss-1.2.0-py3-none-any.whl size=6745 sha256=75db9cadf912607d38b805b6c6370968916c180df8e13b166269b791fb4ba05c\n","  Stored in directory: /root/.cache/pip/wheels/9f/cb/21/088cbb8211ba7951845e4b1ed6bc9c2328d8d6823e382ad732\n","  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for emoji: filename=emoji-2.2.0-py3-none-any.whl size=234926 sha256=4add973b6b7bda59b1c21822f0d5df73a4c9bd8352830171f9d0852732ed48c7\n","  Stored in directory: /root/.cache/pip/wheels/86/62/9e/a6b27a681abcde69970dbc0326ff51955f3beac72f15696984\n","  Building wheel for pyclustering (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyclustering: filename=pyclustering-0.10.1.2-py3-none-any.whl size=2395121 sha256=6f8f05f2fbb8b6afff7b9e139530b0d5e1bbcdf207424cb1cc601fe22546050b\n","  Stored in directory: /root/.cache/pip/wheels/dc/25/8b/072b221a5cff4f04e7999d39ca1b6cb5dad702cc3e1da951d4\n","  Building wheel for python-igraph (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for python-igraph: filename=python_igraph-0.10.2-py3-none-any.whl size=9074 sha256=1baca116773f00a3f8c10a9911bd485cbfca59835ccb70d61a0c0deee04f0868\n","  Stored in directory: /root/.cache/pip/wheels/47/93/11/c7c992fab551e7a7fa504097aac4b992dac996aa1c81490afb\n","Successfully built zss emoji pyclustering python-igraph\n","Installing collected packages: texttable, rapidfuzz, igraph, python-igraph, Levenshtein, thresholdclustering, python-Levenshtein, pyclustering, pulp, nf1, markov-clustering, eva-lcd, emoji, dynetx, demon, chinese-whispers, bimlpa, angel-cd, zss, stanza, rouge, cdlib\n","Successfully installed Levenshtein-0.20.8 angel-cd-1.0.3 bimlpa-0.1.2 cdlib-0.2.6 chinese-whispers-0.8.0 demon-2.0.6 dynetx-0.3.1 emoji-2.2.0 eva-lcd-0.1.1 igraph-0.10.2 markov-clustering-0.0.6.dev0 nf1-0.0.4 pulp-2.7.0 pyclustering-0.10.1.2 python-Levenshtein-0.20.8 python-igraph-0.10.2 rapidfuzz-2.13.3 rouge-1.0.1 stanza-1.4.2 texttable-1.6.7 thresholdclustering-1.1 zss-1.2.0\n"]}]},{"cell_type":"code","source":["import tensorflow_datasets as tfds\n","import nltk\n","import pandas as pd\n","import nltk.data\n","from nltk.stem import PorterStemmer\n","from nltk.corpus import stopwords\n","from nltk import word_tokenize, ngrams\n","import re\n","import json\n","#from typing import final\n","import pandas as pd\n","import numpy as np\n","from collections import Counter\n","import networkx as nx\n","from cdlib import evaluation, algorithms, NodeClustering\n","from functools import reduce\n","import stanza\n","from nltk import word_tokenize, ngrams\n","from matplotlib import pyplot as plt\n","from rouge import Rouge\n","from zss import simple_distance, Node\n","import logging"],"metadata":{"id":"XWPQ9cn8pUYJ","executionInfo":{"status":"ok","timestamp":1670299803411,"user_tz":-540,"elapsed":7900,"user":{"displayName":"박준영","userId":"16803711589753004830"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"ff055627-b4c9-4290-b53f-7c161bc51b68"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Note: to be able to use all crisp methods, you need to install some additional packages:  {'leidenalg', 'graph_tool', 'karateclub', 'infomap', 'wurlitzer'}\n","Note: to be able to use all overlapping methods, you need to install some additional packages:  {'karateclub', 'ASLPAw'}\n","Note: to be able to use all bipartite methods, you need to install some additional packages:  {'leidenalg', 'infomap', 'wurlitzer'}\n"]}]},{"cell_type":"code","source":["nltk.download('punkt')\n","nltk.download('stopwords')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VJ9qgwhDq416","executionInfo":{"status":"ok","timestamp":1670299804014,"user_tz":-540,"elapsed":605,"user":{"displayName":"박준영","userId":"16803711589753004830"}},"outputId":"80728898-06b1-4e80-880d-e50cbbe8e168"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["def clean(line):\n","    # line = line.strip().replace(\"newline_char\", \"||||||||||||||||||||||||||||\")\n","    # line = line.strip().replace(\"newline_char\", \" \")\n","    line = line.strip()\n","    line = line.replace(\"( opens in new window )\", \"\")\n","    line = line.replace(\"click to email this to a friend\", \"\")\n","    line = line.replace(\"lick to share on whatsapp\", \"\")\n","    line = line.replace(\"click to share on facebook\", \"\")\n","    line = line.replace(\"share on facebook\", \"\")\n","    line = line.replace(\"click to share on twitter\", \"\")\n","    line = line.replace(\"click to share on pinterest\", \"\")\n","    line = line.replace(\"click to share on tumblr\", \"\")\n","    line = line.replace(\"click to share on google+\", \"\")\n","    line = line.replace(\"feel free to share these resources in your social \"\n","                        \"media networks , websites and other platforms\", \"\")\n","    line = line.replace(\"share share tweet link\", \"\")\n","    line = line.replace(\"e-mail article print share\", \"\")\n","    line = line.replace(\"read or share this story :\", \"\")\n","    line = line.replace(\"share the map view in e-mail by clicking the share \"\n","                        \"button and copying the link url .     embed the map \"\n","                        \"on your website or blog by getting a snippet of html \"\n","                        \"code from the share button .     if you wish to \"\n","                        \"provide feedback or comments on the map , or if \"\n","                        \"you are aware of map layers or other \"\n","                        \"datasets that you would like to see included on our maps , \"\n","                        \"please submit them for our evaluation using this this form .\", \"\")\n","    line = line.replace(\"share this article share tweet post email\", \"\")\n","    line = line.replace(\"skip in skip x embed x share close\", \"\")\n","    line = line.replace(\"share tweet pin email\", \"\")\n","    line = line.replace(\"share on twitter\", \"\")\n","    line = line.replace(\"feel free to weigh-in yourself , via\"\n","                        \"the comments section . and while you ’ \"\n","                        \"re here , why don ’ t you sign up to \"\n","                        \"follow us on twitter us on twitter .\", \"\")\n","    line = line.replace(\"follow us on facebook , twitter , instagram and youtube\", \"\")\n","    line = line.replace(\"follow us on twitter\", \"\")\n","    line = line.replace(\"follow us on facebook\", \"\")\n","    line = line.replace(\"play facebook twitter google plus embed\", \"\")\n","    line = line.replace(\"play facebook twitter embed\", \"\")\n","    line = line.replace(\"enlarge icon pinterest icon close icon\", \"\")\n","    line = line.replace(\"follow on twitter\", \"\")\n","    line = line.replace(\"autoplay autoplay copy this code to your website or blog\", \"\")\n","    line = line.replace(\"|||||\", \"NEWLINE_CHAR\")\n","    return line"],"metadata":{"id":"EoBnY7NmRiz3","executionInfo":{"status":"ok","timestamp":1670299804015,"user_tz":-540,"elapsed":3,"user":{"displayName":"박준영","userId":"16803711589753004830"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","execution_count":5,"metadata":{"id":"r47tRt4ro8VT","executionInfo":{"status":"ok","timestamp":1670299804015,"user_tz":-540,"elapsed":3,"user":{"displayName":"박준영","userId":"16803711589753004830"}}},"outputs":[],"source":["class Data:\n","    def __init__(self, data_dir):\n","        self.data_dir = data_dir\n","        self.data = []\n","        self.data_uncleaned = []\n","        self.data_tokenized = []\n","        self.labels = []\n","        self.stemmed_docs = []\n","        self.ngrams = []\n","        self.char_ngrams = []\n","        #self.paragraphs = []\n","\n","    def read_txt_data(self):\n","        lines = []\n","\n","        tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n","\n","        with open(self.data_dir) as f:\n","            article = clean(f.read())\n","            paragraphs = list(para for para in filter(str.strip, article.split(\"NEWLINE_CHAR\")) if len(para) > 10)\n","\n","            sent_idx = 0\n","\n","            for para_idx, paragraph in enumerate(paragraphs):\n","\n","                lines.append({'type': 'para', 'doc': str(tokenizer.tokenize(paragraph)[0]), 'para_idx': para_idx, 'sent_idx': None})\n","                for line in list(paragraph.split(\".\")):\n","\n","                  if len(line) > 5:\n","                    lines.append({'type': 'sent', 'doc': str(tokenizer.tokenize(line)[0]), 'para_idx': para_idx, 'sent_idx': sent_idx})\n","                    sent_idx += 1\n","\n","        self.data = lines\n","\n","    def para_read_txt_data(self):\n","        lines = []\n","\n","        tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n","        \n","\n","        with open(self.data_dir) as f:\n","            article = clean(f.read())\n","            paragraphs = list(para for para in filter(str.strip, article.split(\"NEWLINE_CHAR\")) if len(para) > 10)\n","\n","            for paragraph in paragraphs:\n","                lines.extend(tokenizer.tokenize(paragraph))\n","                \n","        self.data = lines\n","\n","    def set_data(self):\n","        data = pd.read_csv(self.data_dir)\n","        self.data = data.TITLE[:20]\n","        self.labels = data.CATEGORY\n","\n","    def build_model(self, stem=True, n=2):\n","        dirty_docs = []\n","        cleaned_ngrams = []\n","        data_tokenized = []\n","\n","        cleaned_tokenized_docs = []\n","        stop_words = set(stopwords.words('english'))\n","\n","        data = self.data # list[dict{'type', 'doc', 'para_idx', 'sent_idx'}]\n","        #print(data)\n","\n","        for data_dict in data:\n","            #doc_idx = document\n","            doc_type = data_dict['type']\n","            document = data_dict['doc']\n","            para_idx = data_dict['para_idx']\n","            sent_idx = data_dict['sent_idx']\n","\n","            document = re.sub(r\"[^a-z\\d ]\", \"\", document.lower())\n","            dirty_docs.append({'type': doc_type, 'doc': document, 'para_idx': para_idx, 'sent_idx': sent_idx})\n","\n","            tokens = word_tokenize(document)\n","            cleaned_vec = [w for w in tokens if w.lower() not in stop_words]\n","\n","            cleaned_tokenized_docs.append({'type': doc_type, 'doc': cleaned_vec, 'para_idx': para_idx, 'sent_idx': sent_idx})\n","            if len(cleaned_vec) > 0:\n","              data_tokenized.append({'type': doc_type, 'doc': cleaned_vec, 'para_idx': para_idx, 'sent_idx': sent_idx})\n","            cleaned_ngrams.append(list(ngrams(cleaned_vec, n)))\n","\n","        self.data_tokenized = data_tokenized\n","\n","        self.data_uncleaned = dirty_docs\n","        self.ngrams = cleaned_ngrams\n","        #self.char_ngrams = list(ngrams(\"\".join(self.data).split(), n))\n","\n","        stemmed = []\n","\n","        if stem:\n","            ps = PorterStemmer()\n","\n","            for doc in self.data_tokenized:\n","                stem_doc = []\n","\n","                for word in doc['doc']:\n","                    stem_doc.append(ps.stem(word))\n","\n","                #stemmed['doc'] = stem_doc\n","                stemmed.append({'type': doc['type'], 'doc': stem_doc, 'para_idx': doc['para_idx'], 'sent_idx': doc['sent_idx']})\n","\n","            self.stemmed_docs = stemmed\n","            #print(stemmed)\n","\n","    def para_build_model(self, stem=True, n=2):\n","        dirty_docs = []\n","        cleaned_ngrams = []\n","\n","        cleaned_tokenized_docs = []\n","        stop_words = set(stopwords.words('english'))\n","\n","        for document in self.data:\n","            document = re.sub(r\"[^a-z\\d ]\", \"\", document.lower())\n","            dirty_docs.append(document)\n","            tokens = word_tokenize(document)\n","            cleaned_vec = [w for w in tokens if w.lower() not in stop_words]\n","            cleaned_tokenized_docs.append(cleaned_vec)\n","            cleaned_ngrams.append(list(ngrams(cleaned_vec, n)))\n","\n","        self.data_tokenized = list(\n","            filter(lambda x: len(x) > 0, cleaned_tokenized_docs))\n","\n","        self.data_uncleaned = dirty_docs\n","        self.ngrams = cleaned_ngrams\n","        self.char_ngrams = list(ngrams(\"\".join(self.data).split(), n))\n","        stemmed = []\n","\n","        if stem:\n","            ps = PorterStemmer()\n","\n","            for doc in self.data_tokenized:\n","                stem_doc = []\n","\n","                for word in doc:\n","                    stem_doc.append(ps.stem(word))\n","\n","                stemmed.append(stem_doc)\n","\n","            self.stemmed_docs = stemmed\n","    \n","    def generate_ngrams(self):\n","        ngrs = []\n","\n","        for doc in self.data:\n","            ngrs.append(list(ngrams(doc, 3)))\n","\n","        return list(ngrs)"]},{"cell_type":"code","source":["class GNN:\n","    def __init__(self):\n","        self.data = []\n","        self.G = None\n","        self.G_ngrs = None\n","        self.pos_tokenized = []\n","\n","    def para_set_data(self, dir):\n","        d = Data(dir)\n","        d.para_read_txt_data()\n","        d.para_build_model()\n","        self.data = d.stemmed_docs\n","        self.orig_data = d.data\n","        self.data_uncleaned = d.data_uncleaned\n","        self.data_ngrams = d.ngrams\n","        self.char_ngrams = d.char_ngrams\n","        self.data_tokenized = d.data_tokenized\n","\n","    def set_data(self, dir):\n","        d = Data(dir)\n","        d.read_txt_data()\n","        d.build_model()\n","        self.data = d.stemmed_docs\n","        self.orig_data = d.data\n","        self.data_uncleaned = d.data_uncleaned\n","        self.data_ngrams = d.ngrams\n","        self.char_ngrams = d.char_ngrams\n","        self.data_tokenized = d.data_tokenized\n","\n","    def doc_sim(self, doc1, doc2):\n","        len_d1 = len(doc1)\n","        len_d2 = len(doc2)\n","        sim_words_set = set(doc1).intersection(set(doc2))\n","        sim_words_len = len(list(sim_words_set))\n","\n","        len_d1_log = np.log(len_d1) if len_d1 > 0 else 0\n","        len_d2_log = np.log(len_d2) if len_d2 > 0 else 0\n","\n","        len_sum = len_d1_log+len_d2_log\n","        sim = sim_words_len/len_sum if len_sum > 0.0 and sim_words_len > 0 else 0\n","\n","        return sim\n","\n","    @staticmethod\n","    def get_ted_sim(doc1, doc2):\n","        return 1/(1+simple_distance(doc1, doc2))\n","\n","\n","    def build_multilayer_graph(self, metric, para_weight, threshold=0.00001, lamd1=1, lamd2=0):\n","        # sentence_nodes = [data for data in self.data if data['type'] == 'sent']\n","        # paragraph_nodes = [data for data in self.data if data['type'] == 'para']\n","\n","        sentence_indexes = [data['sent_idx'] for data in self.data if data['sent_idx'] != None]\n","        max_sent_idx = max(sentence_indexes) +1\n","\n","        para_indexes = [data['para_idx'] for data in self.data]\n","        max_para_idx = max(para_indexes) +1\n","\n","        # print('sent: ',max(sentence_indexes), len(sentence_nodes))\n","        # print('para: ',max(para_indexes), len(paragraph_nodes))\n","\n","        #print(len(sentence_nodes))\n","\n","        sentence_similarity_matrix = np.zeros((max_sent_idx, max_sent_idx))\n","\n","        paragraph_similarity_matrix = np.zeros((max_para_idx, max_para_idx))\n","        hyperlink_matrix = np.zeros((max_sent_idx, max_para_idx))\n","\n","        # for i in range(len(sentence_nodes)):\n","        #   for j in range(i+1, len(sentence_nodes)):\n","        #     if sentence_nodes[i]['para_idx'] == sentence_nodes[j]['para_idx']:\n","        #       sentence_similarity_matrix[i][j] = self.doc_sim(sentence_nodes[i]['doc'], sentence_nodes[j]['doc']) * para_weight[sentence_nodes[i]['para_idx']]\n","        #     else:\n","        #       sentence_similarity_matrix[i][j] = self.doc_sim(sentence_nodes[i]['doc'], sentence_nodes[j]['doc'])\n","            \n","        # for i in range(len(paragraph_nodes)):\n","        #   for j in range(i+1, len(paragraph_nodes)):\n","        #     paragraph_similarity_matrix[i][j] = self.doc_sim(paragraph_nodes[i]['doc'], paragraph_nodes[j]['doc'])\n","        \n","        # for i in range(len(sentence_nodes)):\n","        #   for j in range(len(paragraph_nodes)):\n","        #     if self.doc_sim(sentence_nodes[i]['doc'], paragraph_nodes[j]['doc']) > 0.2:\n","        #         hyperlink_matrix[i][j] = 1\n","\n","        #temp = []\n","        for i in range(len(self.data)):\n","          for j in range(i+1, len(self.data)): # i+1\n","            i_type = self.data[i]['type']\n","            j_type = self.data[j]['type']\n","            if i_type == 'sent' and j_type == 'sent':\n","              if self.data[i]['para_idx'] == self.data[j]['para_idx']:\n","                similarity = self.doc_sim(self.data[i]['doc'], self.data[j]['doc']) * para_weight[self.data[i]['para_idx']]\n","                if similarity == 0:\n","                  sentence_similarity_matrix[self.data[i]['sent_idx'], self.data[j]['sent_idx']] = 0.0001\n","                else:\n","                  sentence_similarity_matrix[self.data[i]['sent_idx'], self.data[j]['sent_idx']] = similarity\n","              else:\n","                similarity = self.doc_sim(self.data[i]['doc'], self.data[j]['doc'])\n","                if similarity == 0:\n","                  sentence_similarity_matrix[self.data[i]['sent_idx'], self.data[j]['sent_idx']] = 0.0001\n","                else:\n","                  sentence_similarity_matrix[self.data[i]['sent_idx'], self.data[j]['sent_idx']] = self.doc_sim(self.data[i]['doc'], self.data[j]['doc'])\n","\n","            elif i_type == 'para' and j_type == 'para':\n","              similarity = self.doc_sim(self.data[i]['doc'], self.data[j]['doc']) #* (para_weight[self.data[i]['para_idx']] + para_weight[self.data[j]['para_idx']])/2\n","              if similarity == 0:\n","                paragraph_similarity_matrix[self.data[i]['para_idx']][self.data[j]['para_idx']] = 0.0001\n","              else:\n","                paragraph_similarity_matrix[self.data[i]['para_idx']][self.data[j]['para_idx']] = similarity\n","            \n","            elif i_type == 'sent' and j_type == 'para':\n","              if self.doc_sim(self.data[i]['doc'], self.data[j]['doc']) > 0:\n","                hyperlink_matrix[self.data[i]['sent_idx']][self.data[j]['para_idx']] = 1\n","\n","            elif i_type == 'para' and j_type == 'sent':\n","              if self.doc_sim(self.data[i]['doc'], self.data[j]['doc']) > 0:\n","                hyperlink_matrix[self.data[j]['sent_idx']][self.data[i]['para_idx']] = 1\n","\n","        sent_adj_matrix = sentence_similarity_matrix / np.sum(sentence_similarity_matrix)\n","        para_adj_matrix = (hyperlink_matrix @ paragraph_similarity_matrix @ hyperlink_matrix.T) / np.sum(hyperlink_matrix @ paragraph_similarity_matrix @ hyperlink_matrix.T)\n","        final_adj_matrix = sent_adj_matrix*lamd1 + para_adj_matrix*lamd2\n","\n","        #print(sentence_similarity_matrix.shape, hyperlink_matrix.shape, paragraph_similarity_matrix.shape)\n","        #final_adj_matrix = sentence_similarity_matrix * lamd1 + hyperlink_matrix @ paragraph_similarity_matrix @ hyperlink_matrix.T * lamd2\n","        #print(final_adj_matrix.shape)\n","        #print(np.max(sentence_similarity_matrix* lamd1), np.min(sentence_similarity_matrix* lamd1), np.mean(np.mean(sentence_similarity_matrix* lamd1)))\n","        #print(np.max(hyperlink_matrix @ paragraph_similarity_matrix @ hyperlink_matrix.T* lamd2), np.min(hyperlink_matrix @ paragraph_similarity_matrix @ hyperlink_matrix.T* lamd2), np.mean(hyperlink_matrix @ paragraph_similarity_matrix @ hyperlink_matrix.T* lamd2))\n","        #print(np.mean(sentence_similarity_matrix / np.sum(sentence_similarity_matrix)))\n","        #print(np.mean((hyperlink_matrix @ paragraph_similarity_matrix @ hyperlink_matrix.T) / np.sum(hyperlink_matrix @ paragraph_similarity_matrix @ hyperlink_matrix.T)))\n","        #print(np.mean(sentence_similarity_matrix* lamd1) , np.mean(hyperlink_matrix @ paragraph_similarity_matrix @ hyperlink_matrix.T* lamd2))\n","\n","        G = nx.Graph()\n","        G.add_nodes_from(list(range(max_sent_idx)))\n","\n","        \n","        weights = []\n","        for i in range(final_adj_matrix.shape[0]):\n","          for j in range(final_adj_matrix.shape[1]):\n","            if final_adj_matrix[i, j] != 0:\n","              weights.append(((i, j), final_adj_matrix[i][j]))\n","    \n","\n","        weights = list(sorted(\n","            weights, key=lambda x: x[1], reverse=True if metric == \"overlap\" else False))\n","        cutoff = int(len(weights) * threshold)\n","\n","\n","        #print(weights[:cutoff])\n","        #print('weights: ', len(weights), print(len(weights[:cutoff])))\n","        #print('temp: ', len(temp))\n","\n","        for edge, weight in weights[:cutoff]:\n","            G.add_edge(*edge, weight=weight)\n","\n","        self.G = G\n","\n","  \n","\n","    def build_graph(self, metric, threshold=0.00001):\n","        # G.add_edges_from([(1,2,{'color':'blue'}), (2,3,{'weight':8})])\n","        # G.add_edge(1, 2, weight=4.7 )\n","\n","        nodes = list(range(len(self.data)))\n","        \n","        G = nx.Graph()\n","\n","        G.add_nodes_from(nodes)\n","\n","        if metric == \"ted\":\n","            deps = self.build_dependency_trees()\n","\n","        weights = []\n","\n","        for i in range(0, len(self.data)):\n","            for j in range(i+1, len(self.data)):\n","\n","                if metric == \"overlap\":\n","                    weights.append(((i, j), self.doc_sim(\n","                        self.data[i], self.data[j])))\n","                else:\n","                    weights.append(\n","                        ((i, j), self.get_ted_sim(deps[i], deps[j])))\n","\n","        weights = list(sorted(\n","            weights, key=lambda x: x[1], reverse=True if metric == \"overlap\" else False))\n","        cutoff = int(len(weights) * threshold)\n","\n","        for edge, weight in weights[:cutoff]:\n","            G.add_edge(*edge, weight=weight)\n","\n","        self.G = G\n","\n","    @staticmethod\n","    def log_graphs(graph):\n","        print(\n","            f\"Graph has {graph.number_of_nodes()} nodes, {graph.number_of_nodes()} edges.\")\n","\n","    def build_graph_ngrams(self):\n","\n","        G = nx.MultiGraph()\n","        G.add_nodes_from(list(self.char_ngrams[0]))\n","\n","        for item in self.char_ngrams[1:]:\n","            G.add_node(item[1])\n","\n","        G.add_edges_from(self.char_ngrams)\n","\n","        print(G.nodes)\n","        self.G_ngrs = G\n","        self.log_graphs(self.G_ngrs)\n","\n","    def build_word_graph(self, data):\n","        g = nx.DiGraph()\n","        g.add_nodes_from([\"S\", \"E\"])\n","\n","        last_tokens = []\n","        edges_to_be_added = []\n","\n","        for idx, sentence in enumerate(data):\n","            nodes = list(g.nodes)\n","            tokens = word_tokenize(sentence)\n","\n","            queue = []\n","\n","            for token in tokens:\n","                if token not in queue:\n","                    queue.append(token)\n","\n","            g.add_nodes_from(queue)\n","\n","            edges_to_be_added.append((\"S\", tokens[0]))\n","            edges_to_be_added.append((tokens[-1], 'E'))\n","\n","            ngrams_list = list(ngrams(tokens, 2))\n","            edges_to_be_added.extend(ngrams_list)\n","\n","            last_tokens.append(tokens[0])\n","\n","            edges_to_be_added.append((last_tokens[idx-1], tokens[0]))\n","\n","        unique, cnts = np.unique(edges_to_be_added, return_counts=True, axis=0)\n","\n","        for idx, edge in enumerate(unique):\n","            g.add_edge(*edge, weight=(1/cnts[idx]))\n","\n","        return g\n","\n","    @staticmethod\n","    def sort_clique_items_by_closeness(closeness, cliques):\n","        cands = []\n","\n","        for clique in list(filter(lambda x: len(x) > 1, cliques)):\n","            mapped = map(lambda x: (x, closeness[x]), clique)\n","            top = list(sorted(mapped, key=lambda x: x[1]))\n","\n","            idx = 0\n","            top_cand = top[idx]\n","\n","            while True:\n","                if top_cand not in cands:\n","                    cands.append(top_cand)\n","                    break\n","                else:\n","\n","                    idx += 1\n","                    if idx < len(top):\n","                        top_cand = top[idx]\n","                    else:\n","                        break\n","\n","        return {k: v for k, v in cands}\n","\n","    def summarize(self, sorting_method=\"pagerank\"):\n","        if sorting_method == \"pagerank\":\n","            try:\n","                scores = nx.pagerank(self.G, max_iter=1000)\n","            except:\n","                scores = {}\n","        elif sorting_method == \"hits\":\n","            try:\n","                scores = nx.hits(self.G, max_iter=1000)[0]\n","            except:\n","                scores = {}\n","        elif sorting_method == \"closeness\":\n","            scores = nx.closeness_centrality(self.G)\n","        elif sorting_method == \"betweenness\":\n","            scores = nx.betweenness_centrality(self.G)\n","        elif sorting_method == \"degree\":\n","            scores = nx.degree_centrality(self.G)\n","        elif sorting_method == \"cliques\":\n","            closeness = nx.closeness_centrality(self.G)\n","            cliques = list(nx.find_cliques(self.G))\n","            scores = self.sort_clique_items_by_closeness(closeness, cliques)\n","\n","        sorted_docs = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n","\n","        final_doc = []\n","\n","        #print(self.orig_data)\n","        # i = 0\n","        # for idx, _ in sorted_docs:\n","        #     if self.orig_data[idx]['type'] == 'sent':\n","        #       i+=1\n","        #       final_doc.append(self.orig_data[idx]['doc'])\n","\n","        #       if i == 5:\n","        #         break\n","        #print(self.orig_data[:5])\n","\n","        #self.orig_data = [data in data for self.orig_data if data['type'] == 'sent']\n","        orig_data = [data for data in self.orig_data if data['type'] == 'sent']\n","        num = 5\n","        i = 0\n","        for idx, _ in sorted_docs:\n","            if i == num:\n","              break\n","            try:\n","              final_doc.append(orig_data[idx]['doc'])\n","              i += 1\n","            except:\n","              i -= 1\n","              continue\n","\n","        return \".\".join(final_doc)\n","    \n","    def paragraph_weight(self, sorting_method=\"pagerank\"):\n","        if sorting_method == \"pagerank\":\n","            try:\n","                scores = nx.pagerank(self.G, max_iter=1000)\n","            except:\n","                scores = {}\n","        elif sorting_method == \"hits\":\n","            try:\n","                scores = nx.hits(self.G, max_iter=1000)[0]\n","            except:\n","                scores = {}\n","        elif sorting_method == \"closeness\":\n","            scores = nx.closeness_centrality(self.G)\n","        elif sorting_method == \"betweenness\":\n","            scores = nx.betweenness_centrality(self.G)\n","        elif sorting_method == \"degree\":\n","            scores = nx.degree_centrality(self.G)\n","        elif sorting_method == \"cliques\":\n","            closeness = nx.closeness_centrality(self.G)\n","            cliques = list(nx.find_cliques(self.G))\n","            scores = self.sort_clique_items_by_closeness(closeness, cliques)\n","\n","        sorted_docs = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n","\n","        para_weight = {}\n","\n","        for i, (para_idx, _) in enumerate(sorted_docs):\n","            if i <= len(sorted_docs) / 5:\n","              para_weight[para_idx] = 5\n","            elif i <= len(sorted_docs) / 5 * 2:\n","              para_weight[para_idx] = 4\n","            elif i <= len(sorted_docs) / 5 * 3:\n","              para_weight[para_idx] = 3\n","            elif i <= len(sorted_docs) / 5 * 4:\n","              para_weight[para_idx] = 2\n","            else:\n","              para_weight[para_idx] = 1\n","\n","        return para_weight\n","\n","    def read_docs_by_idx(self, collection):\n","        final_doc = []\n","\n","        for idx in collection:\n","\n","            final_doc.append(self.orig_data[idx])\n","\n","        return \". \".join(final_doc)\n","\n","    @staticmethod\n","    def tree_to_graph(deps):\n","        g = nx.Graph()\n","\n","        heads = []\n","        ids = []\n","\n","        for dep in deps:\n","            heads.append(dep['head'])\n","            ids.append(dep['id'])\n","\n","        g.add_nodes_from(ids)\n","\n","        for idx, _ in enumerate(ids):\n","            if heads[idx] != 0:\n","                g.add_edge(ids[idx], heads[idx])\n","\n","        return g\n","\n","    @staticmethod\n","    def build_zss_tree(deps):\n","        labels = {}\n","        tree = {}\n","        root = None\n","\n","        for dep in deps:\n","            if dep['head'] == 0:\n","                root = dep['id']\n","                labels[dep['id']] = dep['text']\n","\n","            if True:\n","                if dep['head'] in tree:\n","                    tree[dep['head']].append(dep['id'])\n","                else:\n","                    tree[dep['head']] = [dep['id']]\n","\n","                labels[dep['id']] = dep['text']\n","\n","        def iterative(coll):\n","            out = []\n","            for child in coll:\n","                if child in tree:\n","                    out.append(Node(labels[child], iterative(tree[child])))\n","                else:\n","                    out.append(Node(labels[child], []))\n","\n","            return out\n","\n","        if len(deps) > 1:\n","            final_tree = Node(labels[root], iterative(tree[root]))\n","        else:\n","            final_tree = Node(labels[1], [])\n","\n","        return final_tree\n","\n","    def build_dependency_trees(self):\n","        deps = []\n","        term_freqs = []\n","        nlp = stanza.Pipeline(\n","            processors=\"tokenize, pos, lemma, depparse\", tokenize_pretokenized=True, verbose=False)\n","\n","        doc = nlp(self.data_tokenized)\n","\n","        for idx, sent in enumerate(doc.to_dict()):\n","            deps.append(self.build_zss_tree(sent))\n","\n","        return deps\n","\n","    def evaluate(self, methods, true_summary):\n","        results = []\n","\n","        for method in methods:\n","            summary = gnn.summarize(sorting_method=method)\n","            #print(summary)\n","            summary = summary if summary != \"\" else \" \"\n","            results.append(Rouge().get_scores(summary, true_summary))\n","\n","        return results"],"metadata":{"id":"lWFFIKSnqxcq","executionInfo":{"status":"ok","timestamp":1670299804357,"user_tz":-540,"elapsed":5,"user":{"displayName":"박준영","userId":"16803711589753004830"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["\n","\n","methods = [\"pagerank\", \"closeness\"]\n","for i in range(2):\n","    print(i)\n","    gnn = GNN()\n","    gnn.para_set_data(f'./drive/MyDrive/multi_news/{i}_article.txt')\n","    gnn.build_graph(metric=\"overlap\", threshold=0.1)\n","\n","    f = open(f\"./drive/MyDrive/multi_news/{i}_summ.txt\").readlines()\n","\n","    ## select n most important paragraphs\n","    para_weight = gnn.paragraph_weight(sorting_method='pagerank')\n","    print(para_weight)\n","\n","    gnn = GNN()\n","    gnn.set_data(f'./drive/MyDrive/multi_news/{i}_article.txt')\n","\n","    #raise NotImplementedError\n","\n","    lamd1 = 1\n","    lamd2 = 0\n","    gnn.build_multilayer_graph(metric=\"overlap\",\n","                               threshold=0.1,\n","                               para_weight = para_weight,\n","                               lamd1 = lamd1,\n","                               lamd2 = lamd2)\n","\n","    result = gnn.evaluate(methods, f[0])\n","    print(result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H0kZR3MHiZ1A","executionInfo":{"status":"ok","timestamp":1670299805678,"user_tz":-540,"elapsed":1325,"user":{"displayName":"박준영","userId":"16803711589753004830"}},"outputId":"c963ba63-5711-4737-8455-e85f1f246deb"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["0\n","{7: 5, 52: 5, 40: 5, 83: 5, 11: 5, 56: 5, 8: 5, 53: 5, 6: 5, 51: 5, 79: 5, 34: 5, 4: 5, 49: 5, 47: 5, 2: 5, 76: 5, 31: 5, 81: 4, 21: 4, 66: 4, 3: 4, 18: 4, 63: 4, 44: 4, 87: 4, 48: 4, 13: 4, 58: 4, 72: 4, 27: 4, 16: 4, 61: 4, 5: 4, 17: 4, 24: 4, 28: 3, 29: 3, 35: 3, 42: 3, 50: 3, 62: 3, 69: 3, 73: 3, 74: 3, 80: 3, 85: 3, 19: 3, 64: 3, 70: 3, 25: 3, 10: 3, 55: 3, 22: 2, 67: 2, 57: 2, 12: 2, 23: 2, 68: 2, 30: 2, 75: 2, 38: 2, 14: 2, 59: 2, 37: 2, 41: 2, 84: 2, 43: 2, 86: 2, 0: 2, 45: 2, 15: 1, 60: 1, 32: 1, 77: 1, 33: 1, 78: 1, 9: 1, 54: 1, 20: 1, 65: 1, 82: 1, 71: 1, 26: 1, 36: 1, 39: 1, 1: 1, 46: 1}\n","[[{'rouge-1': {'r': 0.2767857142857143, 'p': 0.3522727272727273, 'f': 0.30999999507200005}, 'rouge-2': {'r': 0.061224489795918366, 'p': 0.07563025210084033, 'f': 0.06766916798773284}, 'rouge-l': {'r': 0.25, 'p': 0.3181818181818182, 'f': 0.2799999950720001}}], [{'rouge-1': {'r': 0.23214285714285715, 'p': 0.36619718309859156, 'f': 0.28415300071545885}, 'rouge-2': {'r': 0.047619047619047616, 'p': 0.08333333333333333, 'f': 0.060606055977961794}, 'rouge-l': {'r': 0.20535714285714285, 'p': 0.323943661971831, 'f': 0.2513661154695572}}]]\n","1\n","{23: 5, 10: 5, 18: 5, 7: 5, 27: 5, 3: 5, 11: 5, 14: 4, 16: 4, 20: 4, 29: 4, 17: 4, 28: 4, 26: 3, 24: 3, 8: 3, 1: 3, 4: 3, 12: 3, 0: 2, 2: 2, 5: 2, 6: 2, 9: 2, 13: 2, 15: 1, 19: 1, 21: 1, 22: 1, 25: 1}\n","[[{'rouge-1': {'r': 0.29310344827586204, 'p': 0.40476190476190477, 'f': 0.33999999512800005}, 'rouge-2': {'r': 0.0718562874251497, 'p': 0.09836065573770492, 'f': 0.08304497782018921}, 'rouge-l': {'r': 0.23275862068965517, 'p': 0.32142857142857145, 'f': 0.26999999512800005}}], [{'rouge-1': {'r': 0.29310344827586204, 'p': 0.40476190476190477, 'f': 0.33999999512800005}, 'rouge-2': {'r': 0.0718562874251497, 'p': 0.09836065573770492, 'f': 0.08304497782018921}, 'rouge-l': {'r': 0.23275862068965517, 'p': 0.32142857142857145, 'f': 0.26999999512800005}}]]\n"]}]},{"cell_type":"code","source":["from tqdm import tqdm\n","for l in [0.2]:\n","  lamd1 = l\n","  lamd2 = 1 - l\n","  batch_size = 30\n","\n","  print(\"================================\")\n","  print('lamds: ', lamd1, lamd2)\n","\n","  methods = [\"pagerank\"] #, \"closeness\", \"hits\", \"betweenness\", \"degree\", \"cliques\"\n","  results = [[] for _ in range(len(methods))]\n","\n","  logging.basicConfig(filename='results2.log',\n","                      level=logging.DEBUG, format='%(asctime)s %(message)s')\n","\n","  batches = 100\n","\n","  avg_results_f = [[[], [], []] for _ in range(len(methods))]\n","  avg_results_r = [[[], [], []] for _ in range(len(methods))]\n","\n","  # for testing all at once and evaluating all at once\n","\n","  for i in tqdm(range(3001)):\n","      #print(i)\n","      # measure paragraph importance weight\n","      para_gnn = GNN()\n","      para_gnn.para_set_data(f'./drive/MyDrive/multi_news/{i}_article.txt')\n","      para_gnn.build_graph(metric=\"overlap\", threshold=0.1)\n","\n","      f = open(f\"./drive/MyDrive/multi_news/{i}_summ.txt\").readlines()\n","\n","      para_weight = para_gnn.paragraph_weight(sorting_method='pagerank') #para idx:para_importance_weight\n","\n","      #print(para_weight)\n","      \n","      gnn = GNN()\n","      gnn.set_data(f'./drive/MyDrive/multi_news/{i}_article.txt')\n","\n","      #raise NotImplementedError\n","\n","      gnn.build_multilayer_graph(metric=\"overlap\",\n","                                threshold=0.1,\n","                                para_weight = para_weight,\n","                                lamd1 = lamd1,\n","                                lamd2 = lamd2)\n","\n","      result = gnn.evaluate(methods, f[0])\n","      #print(result)\n","\n","      logging.info(result)\n","\n","      for idx in range(len(methods)):\n","          results[idx].append(result[idx])\n","\n","  for idx in range(len(methods)):\n","      r1, r2, rl = [], [], []\n","      f1, f2, fl = [], [], []\n","\n","      for item in results[idx]:\n","          r1_score = item[0]['rouge-1']['r']\n","          r2_score = item[0]['rouge-2']['r']\n","          rl_score = item[0]['rouge-l']['r']\n","\n","          if r1 != 0.0:\n","              r1.append(r1_score)\n","\n","          if r2 != 0.0:\n","              r2.append(r2_score)\n","\n","          if rl != 0.0:\n","              rl.append(rl_score)\n","\n","          f1_score = item[0]['rouge-1']['f']\n","          f2_score = item[0]['rouge-2']['f']\n","          fl_score = item[0]['rouge-l']['f']\n","\n","          if f1 != 0.0:\n","              f1.append(f1_score)\n","\n","          if f2 != 0.0:\n","              f2.append(f2_score)\n","\n","          if fl != 0.0:\n","              fl.append(fl_score)\n","\n","      r1 = sum(r1)/len(r1)\n","      r2 = sum(r2)/len(r2)\n","      rl = sum(rl)/len(rl)\n","\n","      f1 = sum(f1)/len(f1)\n","      f2 = sum(f2)/len(f2)\n","      fl = sum(fl)/len(fl)\n","\n","      print(methods[idx], f\"{r1} / {r2} / {rl} \\n\", f\"{f1} / {f2} / {fl} \\n\\n\")\n","\n","\n","  # for testing in different batches\n","\n","  for size in tqdm(range(1, batches + 1)):\n","\n","      for i in range((size-1)*batch_size, ((size-1)*batch_size)+batch_size):\n","          # print(i)\n","          gnn = GNN()\n","          gnn.set_data(f'./drive/MyDrive/multi_news/{i}_article.txt')\n","          gnn.build_graph(metric=\"overlap\", threshold=0.1)\n","\n","          f = open(f\"./drive/MyDrive/multi_news/{i}_summ.txt\").readlines()\n","          result = gnn.evaluate(methods, f[0])\n","          logging.info(result)\n","\n","          for idx in range(len(methods)):\n","              results[idx].append(result[idx])\n","\n","      for idx in range(len(methods)):\n","          r1, r2, rl = [], [], []\n","          f1, f2, fl = [], [], []\n","\n","          for item in results[idx]:\n","              r1_score = item[0]['rouge-1']['r']\n","              r2_score = item[0]['rouge-2']['r']\n","              rl_score = item[0]['rouge-l']['r']\n","\n","              if r1 != 0.0:\n","                  r1.append(r1_score)\n","\n","              if r2 != 0.0:\n","                  r2.append(r2_score)\n","\n","              if rl != 0.0:\n","                  rl.append(rl_score)\n","\n","              f1_score = item[0]['rouge-1']['f']\n","              f2_score = item[0]['rouge-2']['f']\n","              fl_score = item[0]['rouge-l']['f']\n","\n","              if f1 != 0.0:\n","                  f1.append(f1_score)\n","\n","              if f2 != 0.0:\n","                  f2.append(f2_score)\n","\n","              if fl != 0.0:\n","                  fl.append(fl_score)\n","\n","          r1 = sum(r1)/len(r1)\n","          r2 = sum(r2)/len(r2)\n","          rl = sum(rl)/len(rl)\n","\n","          f1 = sum(f1)/len(f1)\n","          f2 = sum(f2)/len(f2)\n","          fl = sum(fl)/len(fl)\n","\n","          avg_results_r[idx][0].append(r1)\n","          avg_results_r[idx][1].append(r2)\n","          avg_results_r[idx][2].append(rl)\n","\n","          avg_results_f[idx][0].append(f1)\n","          avg_results_f[idx][1].append(f2)\n","          avg_results_f[idx][2].append(fl)\n","\n","  for idx in range(len(methods)):\n","      print(\"f for\", methods[idx], sum(avg_results_f[idx][0])/len(avg_results_f[idx][0]), sum(avg_results_f[idx]\n","                                                                                              [1])/len(avg_results_f[idx][1]), sum(avg_results_f[idx][1])/len(avg_results_r[idx][1]))\n","      print(\"r for\", methods[idx], sum(avg_results_r[idx][0])/len(avg_results_r[idx][0]), sum(avg_results_r[idx]\n","                                                                                              [1])/len(avg_results_r[idx][1]), sum(avg_results_r[idx][2])/len(avg_results_r[idx][2]))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0a6Ac1Z2rAmc","outputId":"95358f38-6112-441e-c098-65a44cad42b4","executionInfo":{"status":"ok","timestamp":1670311244347,"user_tz":-540,"elapsed":1959145,"user":{"displayName":"박준영","userId":"16803711589753004830"}}},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["================================\n","lamds:  0.2 0.8\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3001/3001 [21:20<00:00,  2.34it/s]\n"]},{"output_type":"stream","name":"stdout","text":["pagerank 0.23529089538374062 / 0.07169775263483197 / 0.2067631024709254 \n"," 0.2804930232049544 / 0.08524969711883346 / 0.24687648257708739 \n","\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 100/100 [11:17<00:00,  6.78s/it]"]},{"output_type":"stream","name":"stdout","text":["f for pagerank 0.26862098515365257 0.08104979765925135 0.08104979765925135\n","r for pagerank 0.22020368084670253 0.06638719362459866 0.1942800363464616\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]}]}